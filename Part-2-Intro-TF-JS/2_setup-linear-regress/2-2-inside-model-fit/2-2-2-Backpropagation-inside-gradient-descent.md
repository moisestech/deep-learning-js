# ðŸª€ Backprop > Gradient Descent

## [**2.2.2** Backpropagation: Inside gradient descent](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-2/125)

---

### [**Figure 2.9** Illustrating the backpropagation algorithm through a simple linear model with only one updatable weight](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-2/ch02fig09)

### [**Figure 2.10** Schematic drawing showing backpropagation from loss to two updatable weights (k and b)](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-2/ch02fig10)

---

## **Vocabulary**

- **directions** - which axis and direction does gradient descent move towards as the weights are being computed.
- **backpropagation** - The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.
- **`tf.Model.fit()`** -
- **squared error** -
- **labeled** -
- **gradient of loss with respect to V** -
- **opposite** -
- **chain rule** -
- **multiple input features** -
- **basics** -

---

from [[_2-2-inside-model-fit]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[_2-2-inside-model-fit]: _2-2-inside-model-fit.md "ðŸª€ Inside Model Fit"
[//end]: # "Autogenerated link references"
