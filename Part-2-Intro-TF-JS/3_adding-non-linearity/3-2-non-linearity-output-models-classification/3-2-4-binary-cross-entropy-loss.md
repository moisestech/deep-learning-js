# ♒️ Binary Cross Entropy

## [**3.2.4.** Binary cross entropy: The loss function for binary classification](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-3/174)

---

### [**Figure 3.7** The step function used to convert the probability output of a binary-classification model is differentiable almost everywhere]()

### [**Figure 3.8** The binary cross-entropy loss function]()

### [**Table 3.4** Comparing values of binary cross entropy and MSE for hypothetical binary classification results]()

---

## **Vocabulary**

- **entropy** - Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.
  - This is called Information Gain.
  - The greater the reduction in this uncertainty, the more information is gained about Y from X.
  - Entropy, as it relates to machine learning, is a measure of the randomness in the information being processed.
  - The higher the entropy, the harder it is to draw any conclusions from that information.
- [**`binaryCrossentropy`**](https://js.tensorflow.org/api/latest/#metrics.binaryCrossentropy) - Binary cross-entropy metric function.
  - Parameters:
    - yTrue (tf.Tensor) Binary Tensor of truth.
    - yPred (tf.Tensor) Binary Tensor of prediction, probabilities for the 1 case.
    - Returns: tf.Tensor
- **thresholding function** -
- **step function** -
- **limitations** -
- **epsilon, fudge factor** -
- **`truthLabel`** -
- **`measSquaredError`** -

---

from [[_3_adding-non-linearity]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[_3_adding-non-linearity]: ../_3_adding-non-linearity.md "♒️ NON-LINEARITY"
[//end]: # "Autogenerated link references"
