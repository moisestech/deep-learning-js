# ♒️ ROC Trade-Offs

## [**3.2.3.** The ROC curve: Showing trade-offs in binary classification](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-3/148)

---

### [**Figure 3.6** A set of sample ROCs plotted during the training of the phishing-detection model]()

### [**Table 3.3** Commonly seen metrics for a binary-classification problem]()

---

## **Vocabulary**

- **receiver operating characteristic** - is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. [Video, Serrano.Academy](https://www.youtube.com/watch?v=z5qA9qZMyw0)
- **axis** -
- [**false positive rate (FPR)**](https://en.wikipedia.org/wiki/False_positive_rate) - In statistics, when performing multiple comparisons, a false positive ratio (also known as fall-out or false alarm ratio) is the probability of falsely rejecting the null hypothesis for a particular test.
  - The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification).
  - The false positive rate (or "false alarm rate") usually refers to the expectancy of the false positive ratio.
- **true positive rate (TRP)** -
- **false alarm** -
- **`onEpochBegin`** -
- **`Model.fit()`** -
- **`drawROC()`** -
- **under the curve (AUC)** -

---

from [[_3_adding-non-linearity]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[_3_adding-non-linearity]: ../_3_adding-non-linearity.md "♒️ NON-LINEARITY"
[//end]: # "Autogenerated link references"
