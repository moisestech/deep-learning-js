# ♒️ Measure Precision

## [**3.2.2.** Measuring the quality of binary classifiers: Precision, recall, a- accuracy, and ROC curves](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-3/123)

---

### [**Figure 3.5** An example result from a round of training the model for phishing web page detection]()

---

## **Vocabulary**

- **True positive (TPs)** — is an outcome where the model **correctly predicts the positive class**.
- **True negatives (TNs)** — is an outcome where the model **correctly predicts the negative class**.
- **False positives (FPs)** — is an outcome where the model **incorrectly predicts the positive class**.
- **False negatives (FNs)** — is an outcome where the model **incorrectly predicts the negative class**.
- **confusion matrix** —
- **accuracy** —
- **sounds** —
- **precision** —
- **recall** — is the ratio of actual positive examples that are classified by the model as positive.
- **review** —
- **precision** —
- **Precision at X% Recall** —
- **trade-off** -
- **ROC** -

## **Blogs**

- [Classification: True vs. False and Positive vs. Negative](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative)
- [Machine Learning Metrics: The Confusion Matrix, Accuracy, Precision & Recall](https://www.youtube.com/watch?v=_y-peoToPj0)

---

[[_3_adding-non-linearity]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[_3_adding-non-linearity]: ../_3_adding-non-linearity.md "♒️ NON-LINEARITY"
[//end]: # "Autogenerated link references"
