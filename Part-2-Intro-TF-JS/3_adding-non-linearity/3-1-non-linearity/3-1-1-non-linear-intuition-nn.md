# Non-Linear Intuition NNs

## [**3.1.1.** Building the intuition for nonlinearity in neural networks](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-3/23)

---

### [**Figure 3.1** The linear-regression model (panel A) and two-layer neural network (panel B) created for the Boston-housing dataset]()

### [**Figure 3.2** Two frequently used nonlinear activation functions for deep neural networks]()

### [**Figure 3.3** Cascading linear functions (top) and nonlinear functions (bottom)]()

### [**Figure 3.4** Comparing the training results with (panel A) and without (panel B) the sigmoid activation]()

---

## **Vocabulary**

- **key** -
- **activation function**
  - specified by activation: 'sigmoid' in the code), which is represented by the square boxes in panel B of figure 3.1.
  - An activation function[1] is an element-by-element transform.
  - The sigmoid function is a “squashing” nonlinearity, in the sense that it “squashes” all real values from –infinity to +infinity into a much smaller range (0 to +1, in this case).
- [**2D Tensor**](https://js.tensorflow.org/api/0.6.1/#tensor2d)
  - Creates a tf.Tensor with the provided values, shape and dtype.
- **matrix**
- **array**
- **activation function**
- **action potentials**
- **synapses**
- **non-linear**
- **backpropagation**
- **deep-neural networks**
- **sigmoid function**
- **deep-learning**
- **capacity**
- **Boston-housing problem**
- **`multiLayerPerceptronRegressionModel2Hidden()`**
- **Repeating**
- **stacking layers**
- **model interpretability**
- **API for accessing weight values**
- **`getWeights()`**
- **`model.fit()`**
- **tensors**
- **space**
- **decision trees**

---

from [[_3_adding-non-linearity]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[_3_adding-non-linearity]: ../_3_adding-non-linearity.md "♒️ NON-LINEARITY"
[//end]: # "Autogenerated link references"
