# ♒️ Hyper Param Optim

## [**3.1.2.** Hyperparameters and hyperparameter optimization](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-3/67)

- [Initializing neural networks](https://www.deeplearning.ai/ai-notes/initialization/)

---

## **Vocabulary**

- **`leCunNormal`** - these initializations produce weights that are randomly selected numbers multiplied with the variance 1/fan-in.
- **`'glorotNormal'`** - in this approach, each randomly generated weight is multiplied by variance 2/(fan-in + fan-out). For a theoretical justification of the Xavier initialization, you can refer to the deeplearning.ai post on Initialization.
- **model quality** -
- **hyperparameters** -
- **`Model.fit()`** -
- **units** -
- **kernelInitializer** -
- **optimizer** -
- **`Model.compile()`** -
- **weight regularization** -
- **dropout layers** -
- **`'sgd'` vs `'adam'`** -
- **hyperparameter optimization** -
- **hyperparameter tuning** -
- **algorithm** -
- **categorical parameter** -
- **regularization factors** -
- **gradient descent** -

---

from [[_3_adding-non-linearity]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[_3_adding-non-linearity]: ../_3_adding-non-linearity.md "♒️ NON-LINEARITY"
[//end]: # "Autogenerated link references"
