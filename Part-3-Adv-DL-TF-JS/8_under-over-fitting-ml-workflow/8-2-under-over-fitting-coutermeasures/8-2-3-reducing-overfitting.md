# ðŸ’§ Reducing Overfitting

## [**8.2.3.** Reducing overfitting with weight regularization and visualizing it working](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-8/52)

---

## [**Figure 8.5.** Distribution of the values in the kernel with (panel A) and without (panel B) L2 regularization.](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-8/ch08fig05)

## [**Figure 8.6.** A schematic diagram showing the loss curves from simplified cases of underfitting (panel A), overfitting (panel B), and just-right fitting (panel C) in model training.](https://livebook.manning.com/book/deep-learning-with-javascript/chapter-8/ch08fig06)

---

## **Vocabulary**

- **weight regularization**
- **convnet**
- **dropout layers** - is a technique applied to neural networks that randomly sets some of the neuronsâ€™ outputs to zero during training. This forces the network to learn better representations of the data by preventing complex interactions between the neurons: Each neuron needs to learn useful features.
- **`buildMLPModel()`**
- **tf.regularizers.l2()**
- **`l2Rate * l2(kernel)`**
- **hyperparameter**
- **tfvis.show.layer()**
- **quality**
- **tunable parameters**

---

from [[_8-2-under-over-fitting-countermeasures]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[_8-2-under-over-fitting-countermeasures]: _8-2-under-over-fitting-countermeasures.md "ðŸ’§ Under Over Fit Counter Measures"
[//end]: # "Autogenerated link references"
